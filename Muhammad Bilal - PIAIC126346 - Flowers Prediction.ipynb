{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "coordinated-burton",
   "metadata": {},
   "source": [
    "# Muhammad Bilal - PIAIC126346 - Flowers Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "determined-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from  matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpg\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "electronic-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path = 'E:/flowers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "automated-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_transform(Path):\n",
    "    Class = []\n",
    "    Data = []\n",
    "\n",
    "    for direct in os.listdir(Path):\n",
    "        for file in os.listdir(os.path.join(Path, direct)):\n",
    "            try:\n",
    "                image_path = os.path.join(Path, direct, file)\n",
    "                image = tf.io.read_file(image_path)\n",
    "                image = tf.io.decode_image(image, channels=3)\n",
    "                image = tf.image.rgb_to_grayscale(image)\n",
    "                image = tf.image.resize(image,(180,180))\n",
    "                image = tf.cast(image / 255. , tf.float32)\n",
    "                Data.append(image)\n",
    "                Class.append(direct)\n",
    "            except Exception as e:  \n",
    "                pass\n",
    "\n",
    "    return tf.stack(Data, axis = 0), Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "resident-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_data , Classes = img_transform(Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "residential-fiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4323\n",
      "['daisy' 'dandelion' 'rose' 'sunflower' 'tulip']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(Image_data))\n",
    "print(np.unique(Classes))\n",
    "print(len(np.unique(Classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-insight",
   "metadata": {},
   "source": [
    "##### Label Encoding : {'daisy':0 , 'dandelion':1 , 'rose':2 , 'sunflower':3 , 'tulip':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "exposed-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "Classes = lb.fit_transform(Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anonymous-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Image_data.numpy()\n",
    "Y = np.array(Classes)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, Y, train_size=0.6, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "canadian-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2593, 180, 180, 1)\n",
      "(1730, 180, 180, 1)\n",
      "(2593,)\n",
      "(1730,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wrapped-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = train_data.shape[1:]))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "surprised-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop' , \n",
    "              loss = 'sparse_categorical_crossentropy' , \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "devoted-fields",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41/41 [==============================] - 125s 3s/step - loss: 14.3858 - accuracy: 0.2229\n",
      "Epoch 2/50\n",
      "41/41 [==============================] - 117s 3s/step - loss: 4.0844 - accuracy: 0.2214\n",
      "Epoch 3/50\n",
      "41/41 [==============================] - 121s 3s/step - loss: 1.9261 - accuracy: 0.2337\n",
      "Epoch 4/50\n",
      "41/41 [==============================] - 118s 3s/step - loss: 1.7704 - accuracy: 0.2437\n",
      "Epoch 5/50\n",
      "41/41 [==============================] - 120s 3s/step - loss: 1.6026 - accuracy: 0.2827\n",
      "Epoch 6/50\n",
      "41/41 [==============================] - 119s 3s/step - loss: 1.5345 - accuracy: 0.3247\n",
      "Epoch 7/50\n",
      "41/41 [==============================] - 117s 3s/step - loss: 1.5090 - accuracy: 0.3614\n",
      "Epoch 8/50\n",
      "41/41 [==============================] - 121s 3s/step - loss: 1.4457 - accuracy: 0.3810\n",
      "Epoch 9/50\n",
      "41/41 [==============================] - 120s 3s/step - loss: 1.4179 - accuracy: 0.3992\n",
      "Epoch 10/50\n",
      "41/41 [==============================] - 109s 3s/step - loss: 1.3693 - accuracy: 0.4312\n",
      "Epoch 11/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 1.3237 - accuracy: 0.4535\n",
      "Epoch 12/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 1.2628 - accuracy: 0.4875\n",
      "Epoch 13/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 1.2455 - accuracy: 0.5102\n",
      "Epoch 14/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 1.1729 - accuracy: 0.5403\n",
      "Epoch 15/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 1.1270 - accuracy: 0.5646\n",
      "Epoch 16/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 1.0842 - accuracy: 0.5974\n",
      "Epoch 17/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 1.0375 - accuracy: 0.6194\n",
      "Epoch 18/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.9887 - accuracy: 0.6398\n",
      "Epoch 19/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.9346 - accuracy: 0.6641\n",
      "Epoch 20/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.8807 - accuracy: 0.6915\n",
      "Epoch 21/50\n",
      "41/41 [==============================] - 106s 3s/step - loss: 0.8160 - accuracy: 0.7146\n",
      "Epoch 22/50\n",
      "41/41 [==============================] - 106s 3s/step - loss: 0.7884 - accuracy: 0.7339\n",
      "Epoch 23/50\n",
      "41/41 [==============================] - 103s 3s/step - loss: 0.7311 - accuracy: 0.7528\n",
      "Epoch 24/50\n",
      "41/41 [==============================] - 103s 3s/step - loss: 0.7259 - accuracy: 0.7651\n",
      "Epoch 25/50\n",
      "41/41 [==============================] - 106s 3s/step - loss: 0.6661 - accuracy: 0.7833\n",
      "Epoch 26/50\n",
      "41/41 [==============================] - 103s 3s/step - loss: 0.6166 - accuracy: 0.7937\n",
      "Epoch 27/50\n",
      "41/41 [==============================] - 108s 3s/step - loss: 0.5996 - accuracy: 0.7944\n",
      "Epoch 28/50\n",
      "41/41 [==============================] - 116s 3s/step - loss: 0.5771 - accuracy: 0.8141\n",
      "Epoch 29/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.5303 - accuracy: 0.8253\n",
      "Epoch 30/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.5051 - accuracy: 0.8407\n",
      "Epoch 31/50\n",
      "41/41 [==============================] - 106s 3s/step - loss: 0.4602 - accuracy: 0.8515\n",
      "Epoch 32/50\n",
      "41/41 [==============================] - 107s 3s/step - loss: 0.4805 - accuracy: 0.8415\n",
      "Epoch 33/50\n",
      "41/41 [==============================] - 109s 3s/step - loss: 0.4288 - accuracy: 0.8635\n",
      "Epoch 34/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.4124 - accuracy: 0.8654\n",
      "Epoch 35/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.4355 - accuracy: 0.8666\n",
      "Epoch 36/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.3675 - accuracy: 0.8874\n",
      "Epoch 37/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.3506 - accuracy: 0.8893\n",
      "Epoch 38/50\n",
      "41/41 [==============================] - 109s 3s/step - loss: 0.3194 - accuracy: 0.8955\n",
      "Epoch 39/50\n",
      "41/41 [==============================] - 108s 3s/step - loss: 0.3501 - accuracy: 0.8897\n",
      "Epoch 40/50\n",
      "41/41 [==============================] - 108s 3s/step - loss: 0.3370 - accuracy: 0.8986\n",
      "Epoch 41/50\n",
      "41/41 [==============================] - 107s 3s/step - loss: 0.3562 - accuracy: 0.8970\n",
      "Epoch 42/50\n",
      "41/41 [==============================] - 107s 3s/step - loss: 0.2902 - accuracy: 0.9101\n",
      "Epoch 43/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.3119 - accuracy: 0.9128\n",
      "Epoch 44/50\n",
      "41/41 [==============================] - 105s 3s/step - loss: 0.2888 - accuracy: 0.9125\n",
      "Epoch 45/50\n",
      "41/41 [==============================] - 106s 3s/step - loss: 0.2447 - accuracy: 0.9206\n",
      "Epoch 46/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.3421 - accuracy: 0.9090\n",
      "Epoch 47/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.2795 - accuracy: 0.9233\n",
      "Epoch 48/50\n",
      "41/41 [==============================] - 115s 3s/step - loss: 0.2782 - accuracy: 0.9244\n",
      "Epoch 49/50\n",
      "41/41 [==============================] - 115s 3s/step - loss: 0.1871 - accuracy: 0.9437\n",
      "Epoch 50/50\n",
      "41/41 [==============================] - 104s 3s/step - loss: 0.2602 - accuracy: 0.9314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c3efb016d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data , train_labels , epochs = 50 , batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "armed-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 18s 331ms/step - loss: 4.2396 - accuracy: 0.3358\n",
      "4.239647388458252 0.3358381390571594\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "print(test_loss,test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pressing-language",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.8161250e-01, 5.1824682e-06, 1.3803978e-02, 3.7029400e-03,\n",
       "        8.7555853e-04],\n",
       "       [5.5349171e-01, 4.4601369e-01, 4.1726819e-04, 4.1587853e-07,\n",
       "        7.6964323e-05],\n",
       "       [9.5034902e-06, 9.9999046e-01, 3.5772059e-11, 1.3166017e-13,\n",
       "        8.5147717e-10],\n",
       "       ...,\n",
       "       [1.3265498e-01, 1.6147555e-04, 5.9819680e-01, 8.7359980e-02,\n",
       "        1.8162671e-01],\n",
       "       [5.8577504e-02, 3.9591447e-01, 6.8707052e-03, 2.0482389e-02,\n",
       "        5.1815492e-01],\n",
       "       [8.2877884e-03, 1.9120088e-02, 5.1174644e-02, 5.4756534e-01,\n",
       "        3.7385210e-01]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prediction = model.predict(test_data)\n",
    "Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
